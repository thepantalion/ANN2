{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7156698,"sourceType":"datasetVersion","datasetId":4133116},{"sourceId":7232999,"sourceType":"datasetVersion","datasetId":4188273},{"sourceId":7233901,"sourceType":"datasetVersion","datasetId":4188878},{"sourceId":7238041,"sourceType":"datasetVersion","datasetId":4191802}],"dockerImageVersionId":30627,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n\n# Load the numpy array from the file\ndata = np.load('/kaggle/input/training/training_data.npy')\nvalid_periods = np.load('/kaggle/input/training/valid_periods.npy')\ndata.shape","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-21T14:21:21.553110Z","iopub.execute_input":"2023-12-21T14:21:21.554117Z","iopub.status.idle":"2023-12-21T14:21:21.962918Z","shell.execute_reply.started":"2023-12-21T14:21:21.554078Z","shell.execute_reply":"2023-12-21T14:21:21.961946Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"(48000, 2776)"},"metadata":{}}]},{"cell_type":"code","source":"# Calculate the number of valid points for each function\nnum_valid_points = valid_periods[:, 1] - valid_periods[:, 0] + 1\n\n# Filter out functions with less than 210 valid points\nfiltered_data = data[num_valid_points >= 219]\n\n# Check the shape of the filtered data\nprint(filtered_data.shape)\ndata = filtered_data","metadata":{"execution":{"iopub.status.busy":"2023-12-21T14:21:21.964858Z","iopub.execute_input":"2023-12-21T14:21:21.965270Z","iopub.status.idle":"2023-12-21T14:21:22.105613Z","shell.execute_reply.started":"2023-12-21T14:21:21.965226Z","shell.execute_reply":"2023-12-21T14:21:22.104607Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"(19826, 2776)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Shuffle the datasets in unison\nperm = np.random.permutation(data.shape[0])\ndata_shuffled = data[perm]\nvalid_periods_shuffled = valid_periods[perm]\n\nK = 7500\n# Split into training and validation sets\nvalidation_data = data_shuffled[:K]\nvalidation_periods = valid_periods_shuffled[:K]\n\ntraining_data = data_shuffled[K:]\ntraining_periods = valid_periods_shuffled[K:]","metadata":{"execution":{"iopub.status.busy":"2023-12-21T14:21:22.106825Z","iopub.execute_input":"2023-12-21T14:21:22.107158Z","iopub.status.idle":"2023-12-21T14:21:22.244738Z","shell.execute_reply.started":"2023-12-21T14:21:22.107130Z","shell.execute_reply":"2023-12-21T14:21:22.243732Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def apply_valid_periods(data, valid_periods):\n    \"\"\"\n    Modify each function in 'data' based on the corresponding 'valid_periods'.\n\n    Parameters:\n    data (numpy.ndarray): Array of functions, shape (n_samples, n_features).\n    valid_periods (numpy.ndarray): Array of valid periods, shape (n_samples, 2).\n\n    Returns:\n    numpy.ndarray: Modified data array.\n    \"\"\"\n    modified_data = np.zeros_like(data)\n    n_samples, n_features = data.shape\n\n    for i in range(n_samples):\n        left, right = valid_periods[i]\n        # Assuming 'left' and 'right' are indices in the range [0, n_features-1]\n        # Adjust them if they are in a different format\n        modified_data[i, left:right+1] = data[i, left:right+1]\n\n    return modified_data\n\n# Apply the function to your datasets\nmodified_training_data = apply_valid_periods(training_data, training_periods)\nmodified_validation_data = apply_valid_periods(validation_data, validation_periods)\n#modified_test_data = apply_valid_periods(test_data, test_periods)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-21T14:21:22.247631Z","iopub.execute_input":"2023-12-21T14:21:22.248288Z","iopub.status.idle":"2023-12-21T14:21:22.433595Z","shell.execute_reply.started":"2023-12-21T14:21:22.248241Z","shell.execute_reply":"2023-12-21T14:21:22.432702Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"modified_training_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-21T14:21:22.434824Z","iopub.execute_input":"2023-12-21T14:21:22.435153Z","iopub.status.idle":"2023-12-21T14:21:22.441681Z","shell.execute_reply.started":"2023-12-21T14:21:22.435126Z","shell.execute_reply":"2023-12-21T14:21:22.440694Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"(12326, 2776)"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\n\ndef extract_non_overlapping_intervals(data, training_periods, interval_length=218):\n    all_intervals = []\n\n    for function, (left, right) in zip(data, training_periods):\n        # Adjust right boundary to ensure intervals fit within the range\n        right = min(right, len(function))\n\n        # Extract intervals within the valid period\n        for start_idx in range(left, right, interval_length):\n            end_idx = min(start_idx + interval_length, right)\n            interval = function[start_idx:end_idx]\n\n            # Pad the interval if it's shorter than the interval length\n            if len(interval) < interval_length:\n                interval = np.pad(interval, (0, interval_length - len(interval)), 'constant')\n\n            all_intervals.append(interval)\n\n    return np.array(all_intervals)\n\n# Example usage\nfinal_training_data = extract_non_overlapping_intervals(modified_training_data, training_periods)\nfinal_validation_data = extract_non_overlapping_intervals(modified_validation_data, validation_periods)\n# final_test_data = extract_non_overlapping_intervals(modified_test_data, test_periods)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-21T14:21:22.443051Z","iopub.execute_input":"2023-12-21T14:21:22.443300Z","iopub.status.idle":"2023-12-21T14:21:23.292222Z","shell.execute_reply.started":"2023-12-21T14:21:22.443278Z","shell.execute_reply":"2023-12-21T14:21:23.291335Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"final_training_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-21T14:21:23.293431Z","iopub.execute_input":"2023-12-21T14:21:23.293732Z","iopub.status.idle":"2023-12-21T14:21:23.300268Z","shell.execute_reply.started":"2023-12-21T14:21:23.293708Z","shell.execute_reply":"2023-12-21T14:21:23.299263Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"(18767, 218)"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf\nimport joblib\nencoder = tf.keras.models.load_model('/kaggle/input/models/encoder_model.h5')\nsvm = joblib.load('/kaggle/input/models/svm_model.joblib')","metadata":{"execution":{"iopub.status.busy":"2023-12-21T14:21:23.301461Z","iopub.execute_input":"2023-12-21T14:21:23.301804Z","iopub.status.idle":"2023-12-21T14:21:23.361213Z","shell.execute_reply.started":"2023-12-21T14:21:23.301779Z","shell.execute_reply":"2023-12-21T14:21:23.360378Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# Assuming final_training_data and final_validation_data have shape (n_samples, 209)\n# and you want to select the first 200 columns\nX_train_modified = final_training_data[:, :200]\nX_val_modified = final_validation_data[:, :200]\n\n\ndef predict_cluster(encoder, svm, data):\n    \"\"\"\n    Predict cluster assignments using an encoder and SVM classifier.\n\n    Parameters:\n    encoder (tf.keras.Model): Trained Keras encoder model.\n    svm (sklearn.svm.SVC or similar): Trained SVM classifier.\n    data (numpy.ndarray): Data to be clustered, shape (n_samples, n_features).\n\n    Returns:\n    numpy.ndarray: Cluster assignments.\n    \"\"\"\n    # Assuming the encoder expects data with an additional dimension\n    data_reshaped = data.reshape((data.shape[0], data.shape[1], 1))\n\n    # Encode the data\n    encoded_data = encoder.predict(data_reshaped)\n\n    # Flatten the encoded data if necessary (depends on encoder's output shape)\n    if len(encoded_data.shape) > 2:\n        encoded_data = encoded_data.reshape((encoded_data.shape[0], -1))\n\n    # Predict probabilities\n    #probabilities = svm.predict_proba(encoded_data)\n\n    # Select the most probable cluster among the first four\n    #cluster_assignments = np.argmax(probabilities[:, :4], axis=1)\n    # Use the SVM classifier to predict clusters\n    cluster_assignments = svm.predict(encoded_data)\n\n    return cluster_assignments\n\n\n# Assuming you have an 'encoder' and a 'classifier'\n# and a function 'predict_cluster' that returns cluster assignments\ncluster_assignments_train = predict_cluster(encoder, svm, X_train_modified)\ncluster_assignments_train[:] = 0\ncluster_assignments_val = predict_cluster(encoder, svm, X_val_modified)\ncluster_assignments_val[:] = 0\n\ndef split_data_by_clusters(original_data, clusters):\n    cluster_data = {}\n    for cluster in set(clusters):\n        cluster_data[cluster] = original_data[clusters == cluster]\n    return cluster_data\n\n# final_training_data and final_validation_data have the original shape (something, 209)\ntraining_data_by_cluster = split_data_by_clusters(final_training_data, cluster_assignments_train)\nvalidation_data_by_cluster = split_data_by_clusters(final_validation_data, cluster_assignments_val)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-21T14:21:23.362278Z","iopub.execute_input":"2023-12-21T14:21:23.362535Z","iopub.status.idle":"2023-12-21T14:21:55.983011Z","shell.execute_reply.started":"2023-12-21T14:21:23.362513Z","shell.execute_reply":"2023-12-21T14:21:55.981908Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"587/587 [==============================] - 1s 1ms/step\n359/359 [==============================] - 0s 1ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom collections import Counter\n\n# Assuming cluster_assignments_train is your array\n# cluster_assignments_train = np.array([...])  # Your array here\n\n# Count occurrences using NumPy\nunique, counts = np.unique(cluster_assignments_train, return_counts=True)\ncluster_counts = dict(zip(unique, counts))\n\n# Alternatively, using collections.Counter\n# cluster_counts = Counter(cluster_assignments_train)\n\nprint(cluster_counts)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-21T14:21:55.986099Z","iopub.execute_input":"2023-12-21T14:21:55.986461Z","iopub.status.idle":"2023-12-21T14:21:55.994132Z","shell.execute_reply.started":"2023-12-21T14:21:55.986430Z","shell.execute_reply":"2023-12-21T14:21:55.992989Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"{0: 18767}\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nfrom keras.optimizers import SGD\nfrom keras.callbacks import EarlyStopping, TerminateOnNaN\nimport tensorflow as tf\n\ndef train_model_for_cluster(full_train_cluster_data, full_val_cluster_data, epochs=30, batch_size=64):\n    \"\"\"\n    Train a neural network model for a specific data cluster, predicting the last 9 points.\n\n    Parameters:\n    full_train_cluster_data (numpy.ndarray): Full training data for the cluster.\n    full_val_cluster_data (numpy.ndarray): Full validation data for the cluster.\n    epochs (int): Number of epochs to train.\n    batch_size (int): Batch size for training.\n\n    Returns:\n    tf.keras.Model: Trained Keras model.\n    \"\"\"\n\n    # Split data into features (X) and labels (y)\n    print(full_train_cluster_data.shape, full_val_cluster_data.shape)\n    X_train_cluster = full_train_cluster_data[:, :-18]\n    y_train_cluster = full_train_cluster_data[:, -18:]\n    X_val_cluster = full_val_cluster_data[:, :-18]\n    y_val_cluster = full_val_cluster_data[:, -18:]\n\n    # Define the model architecture\n    model = Sequential()\n    model.add(LSTM(50, activation='relu', input_shape=(X_train_cluster.shape[1], 1)))\n    model.add(Dense(18))  # Predicting 9 points\n\n    # Compile the model\n    optimizer = SGD(lr=0.0001)  # Using a lower learning rate with gradient clipping\n    model.compile(optimizer=optimizer, loss='mse')\n\n    # Reshape data for LSTM\n    X_train_reshaped = X_train_cluster.reshape((X_train_cluster.shape[0], X_train_cluster.shape[1], 1))\n    X_val_reshaped = X_val_cluster.reshape((X_val_cluster.shape[0], X_val_cluster.shape[1], 1))\n\n    # Define callbacks\n    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n    terminate_on_nan = TerminateOnNaN()\n\n    # Train the model\n    model.fit(X_train_reshaped, y_train_cluster, validation_data=(X_val_reshaped, y_val_cluster), epochs=epochs, batch_size=batch_size, callbacks=[early_stopping, terminate_on_nan])\n\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-12-21T14:37:02.388307Z","iopub.execute_input":"2023-12-21T14:37:02.389045Z","iopub.status.idle":"2023-12-21T14:37:02.400598Z","shell.execute_reply.started":"2023-12-21T14:37:02.389007Z","shell.execute_reply":"2023-12-21T14:37:02.399693Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"model = train_model_for_cluster(training_data_by_cluster[0], validation_data_by_cluster[0])\nmodel.save('model.keras')","metadata":{"execution":{"iopub.status.busy":"2023-12-21T14:37:02.570102Z","iopub.execute_input":"2023-12-21T14:37:02.570530Z","iopub.status.idle":"2023-12-21T15:13:06.865675Z","shell.execute_reply.started":"2023-12-21T14:37:02.570492Z","shell.execute_reply":"2023-12-21T15:13:06.864904Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"(18767, 218) (11462, 218)\nEpoch 1/30\n294/294 [==============================] - 74s 247ms/step - loss: 0.0978 - val_loss: 0.0865\nEpoch 2/30\n294/294 [==============================] - 72s 247ms/step - loss: 0.0797 - val_loss: 0.0735\nEpoch 3/30\n294/294 [==============================] - 72s 245ms/step - loss: 0.0685 - val_loss: 0.0633\nEpoch 4/30\n294/294 [==============================] - 71s 243ms/step - loss: 0.0571 - val_loss: 0.0495\nEpoch 5/30\n294/294 [==============================] - 73s 249ms/step - loss: 0.0411 - val_loss: 0.0350\nEpoch 6/30\n294/294 [==============================] - 74s 251ms/step - loss: 0.0305 - val_loss: 0.0260\nEpoch 7/30\n294/294 [==============================] - 72s 246ms/step - loss: 0.0232 - val_loss: 0.0206\nEpoch 8/30\n294/294 [==============================] - 73s 247ms/step - loss: 0.0190 - val_loss: 0.0174\nEpoch 9/30\n294/294 [==============================] - 72s 245ms/step - loss: 0.0164 - val_loss: 0.0154\nEpoch 10/30\n294/294 [==============================] - 72s 245ms/step - loss: 0.0148 - val_loss: 0.0141\nEpoch 11/30\n294/294 [==============================] - 73s 247ms/step - loss: 0.0137 - val_loss: 0.0132\nEpoch 12/30\n294/294 [==============================] - 73s 248ms/step - loss: 0.0130 - val_loss: 0.0126\nEpoch 13/30\n294/294 [==============================] - 72s 246ms/step - loss: 0.0125 - val_loss: 0.0122\nEpoch 14/30\n294/294 [==============================] - 73s 247ms/step - loss: 0.0121 - val_loss: 0.0118\nEpoch 15/30\n294/294 [==============================] - 73s 248ms/step - loss: 0.0118 - val_loss: 0.0116\nEpoch 16/30\n294/294 [==============================] - 73s 247ms/step - loss: 0.0116 - val_loss: 0.0114\nEpoch 17/30\n294/294 [==============================] - 72s 244ms/step - loss: 0.0114 - val_loss: 0.0112\nEpoch 18/30\n294/294 [==============================] - 72s 245ms/step - loss: 0.0113 - val_loss: 0.0111\nEpoch 19/30\n294/294 [==============================] - 72s 244ms/step - loss: 0.0111 - val_loss: 0.0110\nEpoch 20/30\n294/294 [==============================] - 72s 245ms/step - loss: 0.0110 - val_loss: 0.0109\nEpoch 21/30\n294/294 [==============================] - 72s 245ms/step - loss: 0.0109 - val_loss: 0.0108\nEpoch 22/30\n294/294 [==============================] - 72s 246ms/step - loss: 0.0108 - val_loss: 0.0107\nEpoch 23/30\n294/294 [==============================] - 72s 244ms/step - loss: 0.0108 - val_loss: 0.0106\nEpoch 24/30\n294/294 [==============================] - 72s 244ms/step - loss: 0.0107 - val_loss: 0.0106\nEpoch 25/30\n294/294 [==============================] - 71s 242ms/step - loss: 0.0106 - val_loss: 0.0105\nEpoch 26/30\n294/294 [==============================] - 71s 242ms/step - loss: 0.0106 - val_loss: 0.0105\nEpoch 27/30\n294/294 [==============================] - 71s 243ms/step - loss: 0.0105 - val_loss: 0.0104\nEpoch 28/30\n294/294 [==============================] - 71s 243ms/step - loss: 0.0105 - val_loss: 0.0103\nEpoch 29/30\n294/294 [==============================] - 71s 241ms/step - loss: 0.0104 - val_loss: 0.0103\nEpoch 30/30\n294/294 [==============================] - 71s 241ms/step - loss: 0.0104 - val_loss: 0.0103\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}